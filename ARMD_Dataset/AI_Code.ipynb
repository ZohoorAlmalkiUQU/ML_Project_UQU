{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b934d-874a-4538-ac7b-9d28babdeed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "CSV_FOLDER = \"path/to/your/csv/files\"\n",
    "OUTPUT_FILE = \"merged_antibiotic_resistance_data.csv\"\n",
    "LINKED_FEATURES = ['anon_id', 'pat_enc_csn_id_coded', 'order_proc_id_coded', 'order_time_jittered_utc']\n",
    "\n",
    "# Define all files with their merge keys and optimal dtypes\n",
    "FILES_CONFIG = [\n",
    "    {\n",
    "        \"name\": \"cultures_cohort\",\n",
    "        \"path\": \"microbiology_cultures_cohort.csv\",\n",
    "        \"merge_on\": None,  # Base table\n",
    "        \"dtype\": {\n",
    "            \"anon_id\": \"int32\",\n",
    "            \"pat_enc_csn_id_coded\": \"int32\",\n",
    "            \"order_proc_id_coded\": \"int32\",\n",
    "            \"order_time_jittered_utc\": \"datetime64[ns]\",\n",
    "            \"ordering_mode\": \"category\",\n",
    "            \"culture_description\": \"category\",\n",
    "            \"was_positive\": \"boolean\",\n",
    "            \"organism\": \"category\",\n",
    "            \"antibiotic\": \"category\",\n",
    "            \"susceptibility\": \"category\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ward_info\",\n",
    "        \"path\": \"microbiology_cultures_ward_info.csv\",\n",
    "        \"merge_on\": LINKED_FEATURES[:3],\n",
    "        \"dtype\": {\n",
    "            \"anon_id\": \"int32\",\n",
    "            \"pat_enc_csn_id_coded\": \"int32\",\n",
    "            \"order_proc_id_coded\": \"int32\",\n",
    "            \"order_time_jittered_utc\": \"int32\",\n",
    "            \"hosp_ward_IP\": \"int8\",\n",
    "            \"hosp_ward_OP\": \"int8\",\n",
    "            \"hosp_ward_ER\": \"int8\",\n",
    "            \"hosp_ward_ICU\": \"int8\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"prior_med\",\n",
    "        \"path\": \"microbiology_cultures_prior_med.csv\",\n",
    "        \"merge_on\": ['anon_id', 'order_time_jittered_utc'],\n",
    "        \"dtype\": {\n",
    "            \"anon_id\": \"int32\",\n",
    "            \"medication_name\": \"category\",\n",
    "            \"medication_time_to_culturetime\": \"float32\",\n",
    "            \"medication_category\": \"category\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"microbial_resistance\",\n",
    "        \"path\": \"microbiology_cultures_microbial_resistance.csv\",\n",
    "        \"merge_on\": ['anon_id', 'organism', 'antibiotic'],\n",
    "        \"dtype\": {\n",
    "            \"anon_id\": \"int32\",\n",
    "            \"organism\": \"category\",\n",
    "            \"antibiotic\": \"category\",\n",
    "            \"resistant_time_to_culturetime\": \"float32\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"demographics\",\n",
    "        \"path\": \"microbiology_cultures_demographics.csv\",\n",
    "        \"merge_on\": ['anon_id'],\n",
    "        \"dtype\": {\n",
    "            \"anon_id\": \"int32\",\n",
    "            \"age\": \"category\",\n",
    "            \"gender\": \"int8\"\n",
    "        }\n",
    "    }\n",
    "    # Add configurations for remaining files following same pattern\n",
    "]\n",
    "\n",
    "def optimize_dataframe(df):\n",
    "    \"\"\"Downcast numeric columns and convert objects to category\"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].astype('category')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        elif df[col].dtype == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df\n",
    "\n",
    "def load_in_chunks(file_path, dtype, chunksize=100000):\n",
    "    \"\"\"Load large CSV in optimized chunks\"\"\"\n",
    "    chunks = []\n",
    "    for chunk in tqdm(pd.read_csv(file_path, dtype=dtype, chunksize=chunksize),\n",
    "                     desc=f\"Loading {os.path.basename(file_path)}\"):\n",
    "        chunks.append(optimize_dataframe(chunk))\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Load base table with full dtype specification\n",
    "print(\"Loading base cultures table...\")\n",
    "base_df = load_in_chunks(\n",
    "    os.path.join(CSV_FOLDER, FILES_CONFIG[0][\"path\"]),\n",
    "    dtype=FILES_CONFIG[0][\"dtype\"]\n",
    ")\n",
    "\n",
    "# Sequential merging\n",
    "for config in tqdm(FILES_CONFIG[1:], desc=\"Merging additional files\"):\n",
    "    try:\n",
    "        file_path = os.path.join(CSV_FOLDER, config[\"path\"])\n",
    "        print(f\"\\nProcessing {config['name']}...\")\n",
    "        \n",
    "        # Load with appropriate method based on file size\n",
    "        file_size = os.path.getsize(file_path) / (1024**2)  # in MB\n",
    "        if file_size > 500:  # Use chunking for large files\n",
    "            df_to_merge = load_in_chunks(file_path, config.get(\"dtype\"))\n",
    "        else:\n",
    "            df_to_merge = pd.read_csv(file_path, dtype=config.get(\"dtype\"))\n",
    "            df_to_merge = optimize_dataframe(df_to_merge)\n",
    "        \n",
    "        # Perform the merge\n",
    "        base_df = pd.merge(\n",
    "            base_df,\n",
    "            df_to_merge,\n",
    "            on=config[\"merge_on\"],\n",
    "            how=\"left\",\n",
    "            suffixes=('', f'_{config[\"name\"]}'),\n",
    "            validate=\"one_to_many\"\n",
    "        )\n",
    "        \n",
    "        # Clean up memory\n",
    "        del df_to_merge\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {config['path']}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Final memory optimization\n",
    "print(\"\\nPerforming final memory optimization...\")\n",
    "base_df = optimize_dataframe(base_df)\n",
    "\n",
    "# Save the merged data\n",
    "print(\"\\nSaving merged dataset...\")\n",
    "if len(base_df) > 1000000:  # Chunk output if too large\n",
    "    for i, chunk in enumerate(np.array_split(base_df, 10)):\n",
    "        chunk.to_csv(f\"{OUTPUT_FILE}_{i}.csv\", index=False)\n",
    "else:\n",
    "    base_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"\\nMerge complete. Final shape: {base_df.shape}\")\n",
    "print(\"Sample columns:\", list(base_df.columns)[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
