{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b6a6c6-0b43-480b-8b50-6baed20679d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import dask as d\n",
    "\n",
    "\n",
    "sample_folder = 'new_sample_one_processed/'\n",
    "merged_folder = 'merged_sample_processed/'\n",
    "\n",
    "os.makedirs(merged_folder, exist_ok=True)\n",
    "\n",
    "merged_dtype = files_info[0]['dtype']\n",
    "base_sample = pd.read_parquet(sample_folder + files_info[0]['name'] + '.parquet', engine='pyarrow')\n",
    "base_sample = base_sample.astype(merged_dtype)\n",
    "\n",
    "print(f\"Base file: {sample_folder + 'processed_' + files_info[0]['name'] + '.parquet'} is loaded\")\n",
    "\n",
    "def get_df_size_mb(df):\n",
    "    return df.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "\n",
    "def getDatatype(file):\n",
    "    all_keys = {'anon_id':'object',\n",
    "               'pat_enc_csn_id_coded':'Int64',\n",
    "               'order_proc_id_coded':'Int64',\n",
    "               'order_time_jittered_utc':'object'}\n",
    "    part_keys = {\n",
    "                'anon_id':'object',\n",
    "               'pat_enc_csn_id_coded':'Int64',\n",
    "               'order_proc_id_coded':'Int64'\n",
    "    }\n",
    "    for f in files_info:\n",
    "        if f['name']== file:\n",
    "            if len(f['merge_on']) == 4:\n",
    "                all_keys.update(f['dtype'])\n",
    "                return all_keys\n",
    "            else:\n",
    "                part_keys.update(f['dtype'])\n",
    "                return part_keys\n",
    "\n",
    "\n",
    "MAX_MEMORY_MB = 100\n",
    "mode = 'w'\n",
    "full = False\n",
    "\n",
    "for file in files_info[1:]:\n",
    "    print(f\"Start merge with: {sample_folder +  'processed_'  + file['name'] + '.parquet'} \")\n",
    "    df = pd.read_parquet(sample_folder + file['name'] + '.parquet' )\n",
    "    dtype = getDatatype(file['name'])\n",
    "    merged_dtype.update(dtype)\n",
    "    df = df.astype(dtype)\n",
    "    if get_df_size_mb(base_sample) < MAX_MEMORY_MB and not full: \n",
    "        base_sample = base_sample.merge(df, how = 'inner', on = file['merge_on'] , suffixes=('', '_right'))\n",
    "        base_sample = base_sample.drop(columns=[col for col in base_sample.columns if col.endswith('_right')])\n",
    "        base_sample.to_parquet(merged_folder + 'merged_ARMD.parquet' , engine='pyarrow', index=False)\n",
    "    else: \n",
    "        full = True\n",
    "        base_sample = pd.DataFrame()\n",
    "         \n",
    "    if full: \n",
    "        parquet_file = pq.ParquetFile(merged_folder + 'merged_ARMD.parquet')\n",
    "        for i in range(parquet_file.num_row_groups):\n",
    "            chunk = parquet_file.read_row_group(i).to_pandas()\n",
    "            temp = chunk.merge(df, how = 'inner', on = file['merge_on'], suffixes=('', '_right') )\n",
    "            temp = temp.drop(columns=[col for col in temp.columns if col.endswith('_right')])\n",
    "            temp = temp.astype(merged_dtype)\n",
    "            parquet_path = merged_folder + 'temp_merged_ARMD.parquet'\n",
    "            table = pa.Table.from_pandas(temp)\n",
    "            if os.path.exists(parquet_path) and mode == 'a':\n",
    "                with pq.ParquetWriter(parquet_path, table.schema, compression='snappy', use_dictionary=True) as writer:\n",
    "                    writer.write_table(table)\n",
    "            else:\n",
    "                pq.write_table(table, parquet_path, compression='snappy', use_dictionary=True)\n",
    "                mode = 'a'\n",
    "                \n",
    "        mode = 'w'\n",
    "        \n",
    "        for i in range(parquet_file.num_row_groups):\n",
    "            chunk = parquet_file.read_row_group(i).to_pandas() \n",
    "            chunk = chunk.astype(merged_dtype)\n",
    "            parquet_path = merged_folder + 'temp_merged_ARMD.parquet'\n",
    "            table = pa.Table.from_pandas(chunk)\n",
    "            if os.path.exists(parquet_path) and mode == 'a':\n",
    "                with pq.ParquetWriter(parquet_path, table.schema, compression='snappy', use_dictionary=True) as writer:\n",
    "                    writer.write_table(table)\n",
    "            else:\n",
    "                pq.write_table(table, parquet_path, compression='snappy', use_dictionary=True)\n",
    "                mode = 'a'\n",
    "                   \n",
    "print(\"The merge is done Alhamdulillah :) \")          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
